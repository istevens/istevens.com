title: Realtime raytracing … and a trip down memory lane
---
dek: This is the dek
---
pub_date: 2020-05-25
---
author: Ian Stevens
---
body:

Why raytracing? Photo-realistic with precise light

If you've been following the state of computer graphics, you probably heard
that [realtime raytracing is almost upon
us](https://www.bbc.com/news/business-52541218). Nvidia and AMD will ship
graphics cards with this capability later this year, with Sony and Microsoft
leveraging their capabilities in new Playstation, Xbox, and Windows releases.
Gaming and home entertainment just got real.

If you *don't* follow computer graphics, you're proabably wondering what all
this means. Computer-generated scenes have grown increasingly more realistic,
thanks largely to improvements in computer hardware. Though recent games and
demos appear realistic, they routinely take advantage of optimizations to
subtly fake reality. With closer scrutiny, lighting, shadows, and reflections
can appear slightly off. These scenes lack *photorealism*, the decades-long "holy grail" of
computer graphics.

Since 1980, *raytracing* has been the best promise for photorealism in computer
graphics. The technique involves following (or tracing) rays of light as they
leave a light source and interact with the scene. That light bounces off
many surfaces, each with their own reflective or absorption capabilities.
At every collision, the raytracing algorithm calculates changes in that ray
of light and adjusts accordingly before it's finally projected onto the resulting
2-dimensional image.

As you can imagine, the raytracing process is extremely intensive, often taking
hours for a single frame. That hasn't stopped it from being used in movies and
video, where post-production often takes months. Realtime raytracing, though,
opens up new possibilities. Instead of watching actors in front of a green
screen, a director could watch them in a photorealistic scene live on a
monitor. With realtime raytracing on a chip, movie crews with even the smallest
budgets will be able to track a complete scene while filming.

If you're an avid gamer, realtime raytracing brings an improved look to
many of your games. This will be likely have the most impact in VR, offering
increasingly immersive environments. It will eventually find its way to your
phone, resulting in more natural interactions with AR objects and reality. A
coffee table previewed in your living room will look so real you
might be tempted to put your mug on it.

All this thinking about raytracing reminded me of my days playing around with
[POVray](http://povray.org/) in high-school and university. Simple scenes
took several minutes to render. More complex scenes required more computing
power than I could muster.

I dug up some of my old POVray files to see if I could render them today. What
I found was an old QuickBASIC program generating 1500+ random spheres connected
in a sphere shape. I remember being able to draw a portion of that scene, but
not much else. Luckily there's [a free interpreter and editor for Mac
OS](https://github.com/QB64Team/qb64/releases). Within a few minutes I had the
scene:

<figure>
<img src="first_bubble_render.png"/>
<figcaption>First look of a POVRay scene I created years ago — looks like we need to add light, and maybe zoom out a little.</figcaption>
</figure>

The scene took only seconds to render, so I decided to double the number of
spheres, position the camera and some lights, specify a plane, and add some
colour. It didn't take much time to render the resulting image:

<figure>
<img src="green_bubbles.png"/>
<figcaption>The same scene as the black-and-white render, with light and a plane added, and the camera in a better place.</figcaption>
</figure>

It looks like the sphere I was drawing was actually a half-sphere. This might
have been an old optimization on my part. Rather than fix it, I decided to make
it a feature. I re-used the same object, then rotated and translated the
half-spheres to appear as if it was a sphere opened to the camera:

<figure>
<img src="open_sphere.png"/>
<figcaption>The same green object as above, but two of them with better colour and lighting.</figcaption>
</figure>
