title: Realtime raytracing … and a trip down memory lane
---
dek:

Video games, virtual reality, and augmented reality will appear more
lifelike and naturally lit thanks to upcoming realtime raytracing chips and
consoles. I revisit an old scene created *[mumble-mumble]* years ago in POVRay.

---
pub_date: 2020-05-25
---
author: Ian Stevens
---
body:

Digital gaming and home entertainment is about to get real … well, *more* real,
not *really* real. Computer-generated scenes have grown increasingly
realistic, thanks largely to faster hardware. Though recent
games and demos resemble reality, they subtly fake it by taking advantage of
optimizations and shortcuts. With closer scrutiny, lighting,
shadows, and reflections can appear slightly off. These scenes lack
*photorealism*, the decades-long "holy grail" of computer graphics.

<aside>A coffee table previewed in your living room will look so real you might be tempted to put your mug on it.</aside>

That holy grail of photorealistic game play is within our grasp. NVIDIA and AMD will ship
[graphics processors supporting realtime raytracing later this
year](https://www.bbc.com/news/business-52541218), with Sony and Microsoft
leveraging their capabilities in new Playstation, Xbox, and Windows releases.

Since 1980, *raytracing* has been the best promise for photorealism in computer
graphics. The technique involves following (or tracing) rays of light as they
leave a light source and interact with the scene. That light bounces off
many surfaces, each with their own reflective or absorption capabilities.
At every collision, the raytracing algorithm calculates changes in that ray
of light and adjusts accordingly before it's finally projected onto the resulting
2-dimensional image.

As you can imagine, this process is extremely intensive, often taking
hours for a single frame. That hasn't stopped it from being used in movies and
video, where post-production takes months. Realtime raytracing, though,
opens up new possibilities. Instead of watching actors in front of a green
screen, a director can watch them in a photorealistic scene live on a
monitor. With realtime raytracing on a chip, movie crews with even the smallest
budgets will be able to track a complete scene while filming.

If you're an avid gamer, realtime raytracing brings an improved look to
many of your games. This will likely have the most impact in VR, offering
increasingly immersive environments. It will eventually find its way to your
phone, resulting in more natural interactions with AR objects and reality. A
coffee table previewed in your living room will look so real you
might be tempted to put your mug on it.

All this thinking about raytracing reminded me of high-school and university when I played around with
[POVray](http://povray.org/), an open
source raytracer with its own scene description language. I would later write
my own raytracer for a computer graphics class. Simple scenes took several
minutes to render. More complex scenes required more computing power than I
could muster.

I dug up some of my old POVray files to see if I could render them today. What
I found was an old QuickBASIC program generating 1500+ random spheres connected
in a sphere shape. I remember being able to draw a portion of that scene, but
not much else. Luckily there's [a free interpreter and editor for Mac
OS](https://github.com/QB64Team/qb64/releases). Within a few minutes I had the
scene:

<figure>
<img src="first_bubble_render.png"/>
<figcaption>First look of a POVRay scene I created years ago — looks like we need to add light, and maybe zoom out a little.</figcaption>
</figure>

The scene took only seconds to render, so I decided to double the number of
spheres, position the camera and some lights, specify a plane, and add some
colour. It didn't take much time to render the resulting image:

<figure>
<img src="green_bubbles.png"/>
<figcaption>The same scene as the black-and-white render, with light and a plane added, and the camera in a better place.</figcaption>
</figure>

It looks like the sphere I was drawing was actually a half-sphere. This might
have been an old optimization on my part. Rather than fix it, I decided to make
it a feature. I re-used the same object, then rotated and translated the
half-spheres to appear as if it was a sphere opened to the camera:

<figure>
<img src="open_sphere.png"/>
<figcaption>The same green object as above, but two of them with better colour and lighting.</figcaption>
</figure>
