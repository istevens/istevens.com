title: Confirmation bias part 3
---
slug: confirmation-bias-part-3
---
pub_date: 2019-11-14
---
author: Ian Stevens
---
dek: Dek goes here
---
body:

Confirmation bias can occur at every stage of our learning process, from
initial belief to evidence gathering. At every stage, it reinforces itself and
may become so severe that our bias becomes entrenched. Worse, our internal
reasoning remains intact, so we are unaware of our own confirmation bias. Our
battle with bias may seem hopeless, but there are ways in which we can fight or
lessen it.

Although confirmation bias may seem entrenched in our brains, there are
times where we unknowingly reduce its impact. If we feel we may be punished
for less-than-perfect decisions, our desire for approval can help lessen bias.
"Punishment" could mean a loss of money, a loss of status, or a cost for a bad
decision. Punitive measures are not always available, however. In those
situations, creating an environment which provides a chance to correct and
adjust belief or decisions can also help. [KLAYMAN1995]_

Although costs for bad decisions can help limit bias in some
cases, there is little evidence that incentives improve the reliability of our
decision-making. [LARRICK2004]_ [RABIN1999]_ Incentives might work if we feel
that a given task is boring and would otherwise not put in the effort.
Accountability for our decisions, on the other hand, can be better than incentives at countering bias in tasks
for which we already possess the appropriate strategy, usually due to
experience in a specific subject. We have a strong social need for consistency,
and are willing to put in the effort and more effectively use information when
making decisions. To avoid embarrassment, we are more likely to foresee flaws
with preemptive self-criticism. Our thirst for accountability may go too far,
as we sometimes feel a need to "give people what they want", particularly if we
are undecided. [LARRICK2004]_

Context is also key when making decisions without bias. It helps to have
experience in the area under study, especially if we encounter a problem we
have solved before. Yet confirmation bias often reappears if we try to map
that experience to a different domain. We may also tap into a general schema to
find inconsistencies. Reasoning in areas of duty or obligation — *deontic*
reasoning — such as when a social rule is being broken, can also be relatively
bias-free. [KLAYMAN1995]_

Confirmation bias can sometimes develop if we fail to properly apply formal
reasoning. We may have some basic logic, economics, or statistics knowledge
(such as sampling) but you may not know when or how to use it. If experience
aids to limit confirmation bias, can training help? There is evidence that
short training sessions in a domain with which we're comfortable (such as
sports) can aid us to reduce bias in other areas. That assist, however,
often diminishes over two weeks. [LARRICK2004]_ A more thorough study might be
a better approach, yet little data exists on how specific this training can be
and how generalizable it is. [KLAYMAN1995]_

[Training in biases, rep vs. odds]

As Nelson's analysis of studies on gender and risk shows, even scholars and
experts are often victims of bias. [NELSON2015]_ There seems to be no guarantee
that intuition can be improved with more education. [KLAYMAN1995]_ Outside
motivation can also only go so far, and may sometimes have the opposite effect.
How then, can we hope to lessen our bias? Formal approaches exist but they are
more geared towards reducing bias in group decisions. We cannot debias
ourselves by ourselves, as we likely don't realise our own biases.
[LARRICK2004]_ As it turns out, the most effective strategy for reducing bias
may be to consider the opposite.

If you've debated a position in school – in English or a debate class, perhaps
– you may have prepared by researching an opposing viewpoint. Considering the
opposite can also be a decent strategy for fighting bias in our beliefs. This
may be as simple as asking ourselves how we may be wrong on a position, why,
and for what reasons. This approach can help reduce overconfidence – a symptom
of confirmation bias – and is shown to lessen bias when looking for and
interpreting new information. [LARRICK2004]_ We reason better with two theories
than when evaluating a single hypothesis. Alternative theories can even come
from other sources. What's important is that we seriously examine a specific
opposing belief. [KLAYMAN1995]_

Naturally, *seriously* examining an alternate belief is key. We may not give an
opposing belief its due, especially if we feel ours is already viable.
[KLAYMAN1995]_ Although directing our attention to contrary evidence can help
counter bias, requiring too many opposing viewpoints may backfire. Failing to
come up with a required number of alternate theories might make us more
overconfident in our own. [LARRICK2004]_ Considering more than one theory at
once can also divide our attention. We might prefer to think about alternates
separately and independently. [KLAYMAN1995]_

We may be able to hold our own confirmation bias at bay so long as we are aware
of it, and give serious thought to viewpoints opposed to our own. What about
people that we work with, or our friends?

> But what a fool believes he sees<br/>
> No wise man has the power to reason away<br/>
> What seems to be<br/>
> Is always better than nothing<br/>
> Than nothing at all
>
> [The Doobie Brothers, "What a Fool Believes"](https://youtu.be/Zjqcf5F0YRg)

Unfortunately, when it comes to other individuals, we may just have to grin and
bear it. In the absence of bias, a person could correct their belief with more
information. However, with a person affected by confirmation bias, doing so may
result in the opposite effect, and increase their leanings. Giving the same
ambiguous information to people with differing beliefs may move their beliefs
further apart. [RABIN1999]_ In one study, [cite capital punishment study]
Depending on their viewpoint, others may see the same evidence you do and
interpret differently, judging it as being more consistent with their bias.
[NICKERSON1998]_

Considering belief formation as a series of signals can also show how difficult
it may be to debias someone else. The effect of each signal depends on those
which came before it as well as any prior belief. To debias someone, we may
need to know their initial belief on a topic as well as the order of signals
which followed. [RABIN1999]_
Done carefully, with open minds, and curiosity, finding how they gathered their evidence, as in Getting To Yes.

Our friends and family with severe bias may be lost to it, but our workplace
can still be saved. Decisions made at work have the advantage in that they
often involve groups, which can be more readily debiased than individuals. Many
strategies for lessening bias in groups exist, usually involving a framework or
a tool to help make sound decisions. Groups can make use of decision aids,
information displays, statistical models, and other formal decision analysis
techniques. Complex problems, say, can be split into smaller, simpler ones and
assigned to smaller groups. These technical strategies are simply out of reach
for most people. Whereas we as individuals can introduce bias at every step of
the decision-making process, groups can track their progress and use those
results as feedback.

Adoption can be a problem when using strategies or tools to make unbiased
decisions at work. A bottom-up approach may have better results than a general
process imposed from the top-down. When the people making the decisions choose
a strategy appropriate to their group, their sense of ownership will help them
stick to it and approach it more honestly. Beware, however, as with ourselves,
groups can also underestimate their own bias and be overconfident in their
decision-making. They, like us, may fail to recognize a need for help. [LARRICK2004]_

Groups are also prone to "group-think". Their members may be influenced by
others, and groups may anchor on the judgments of a few people. Having group
members think about their preferences and estimates before a meeting might help
lessen this risk. Tools and strategies can also check errors in the
decision-making process. It is also a good idea to maintain complementary
expertise within the group, and be aware of blind spots due to shared errors. [LARRICK2004]_

Group-think due to blind spots may be lessened through diversity of experience
within the group. While training can help preserve that diversity of
perspectives, groups can do better by increasing the sample size of experience.
[LARRICK2004]_ Drawing people in from a wider community will increase diversity
of experience and may, in turn, increase diversity of thought. To reduce the
risk of locally-held beliefs, groups should bring in members of differing
genders, ethnicities, social-economic class, and nationality. [NELSON2015]_

### References

* <a name="JONES2000"/>[JONES2000] Jones, M., and Sugden, R. (2000). Positive confirmation bias in the acquisition of information. (Dundee Discussion Papers in Economics; No.  115). University of Dundee.
* <a name="LARRICK2004"/>[LARRICK2004] Larrick, R. P. (2004). Debiasing, in Blackwell Handbook of Judgment and Decision Making (eds D. J. Koehler and N. Harvey), Blackwell Publishing Ltd, Malden, MA, USA.
* <a name="NELSON2015"/>[NELSON2015] Nelson, J. A. (2015). Are women really more risk-averse than men? A re-analysis of the literature using expanded methods. Journal of Economic Surveys, 29: 566-585.
* <a name="KLAYMAN1995"/>[KLAYMAN1995] Klayman, J. (1995). Varieties of confirmation bias. In J. Busemeyer, R. Hastie, & D. L. Medin (Eds.), Decision making from a cognitive perspective. New York: Academic Press (Psychology of Learning and Motivation, vol. 32), pp. 365-418.
* <a name="BIAN2017"/>[BIAN2017] Bian, L., Leslie, S., and Cimpian, A. (2017). Gender stereotypes about intellectual ability emerge early and influence children’s interests.  Science, 27 Jan 2017, Vol. 355, Issue 6323, pp. 389-391.
* <a name="NICKERSON1998"/>[NICKERSON1998] Nickerson, J. S. (1998). Confirmation bias: a ubiquitous phenomenon in many guises. Review of General Psychology, Vol. 2, No. 2, pp. 175-220.
* <a name="RABIN1999"/>[RABIN1999] Rabin, Matthew and Schrag, Joel L. (1999). First Impressions Matter: A Model of Confirmatory Bias, The Quarterly Journal of Economics, 114, issue 1, p. 37-82.
